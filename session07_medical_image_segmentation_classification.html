<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Session 07 - Medical Image Segmentation and Classification</title>
  <link href="https://fonts.googleapis.com/css2?family=Raleway:wght@300;400;500;600;700&display=swap" rel="stylesheet">
  <style>
    * { margin: 0; padding: 0; box-sizing: border-box; }
    body { 
      font-family: 'Raleway', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif; 
      background: #f0f0f0; 
      color: #2c3e50; 
      font-size: 15px; 
      line-height: 1.6; 
      font-weight: 400;
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
    }
    .slide { 
      width: 960px; 
      height: 540px; 
      padding: 32px 40px; 
      background: #fff; 
      margin: 20px auto; 
      box-shadow: 0 4px 20px rgba(0,0,0,0.15); 
      position: relative;
      overflow: hidden;
    }
    .slide-title { 
      color: #0000FF; 
      font-size: 26px; 
      font-weight: 400; 
      margin-bottom: 18px; 
      padding-bottom: 8px; 
      border-bottom: 2px solid #0000FF; 
      letter-spacing: 0.3px;
    }
    .two-column { display: flex; gap: 30px; }
    .column { flex: 1; }
    .section-title { 
      color: #0000FF; 
      font-size: 16px; 
      font-weight: 600; 
      margin: 12px 0 8px 0; 
      letter-spacing: 0.2px;
    }
    .blue-box { 
      background: #e6e6ff; 
      border-left: 4px solid #0000FF; 
      padding: 10px 14px; 
      margin: 8px 0; 
      border-radius: 0 3px 3px 0;
    }
    .cream-box { 
      background: #fef9e7; 
      border-left: 4px solid #f4d03f; 
      padding: 10px 14px; 
      margin: 8px 0; 
      border-radius: 0 3px 3px 0;
    }
    .pink-box { 
      background: #fdf2f8; 
      border-left: 4px solid #ec4899; 
      padding: 10px 14px; 
      margin: 8px 0; 
      border-radius: 0 3px 3px 0;
    }
    .green-box { 
      background: #e8f8f5; 
      border-left: 4px solid #1abc9c; 
      padding: 10px 14px; 
      margin: 8px 0; 
      border-radius: 0 3px 3px 0;
    }
    .purple-box { 
      background: #f5eef8; 
      border-left: 4px solid #9b59b6; 
      padding: 10px 14px; 
      margin: 8px 0; 
      border-radius: 0 3px 3px 0;
    }
    .red-box {
      background: #fdeaea;
      border-left: 4px solid #e74c3c;
      padding: 10px 14px;
      margin: 8px 0;
      border-radius: 0 3px 3px 0;
    }
    .gray-box {
      background: #f5f6fa;
      border-left: 4px solid #7f8c8d;
      padding: 10px 14px;
      margin: 8px 0;
      border-radius: 0 3px 3px 0;
    }
    .orange-box {
      background: #fef3e2;
      border-left: 4px solid #e67e22;
      padding: 10px 14px;
      margin: 8px 0;
      border-radius: 0 3px 3px 0;
    }
    table { width: 100%; border-collapse: collapse; margin: 8px 0; font-size: 13px; }
    th { background: #0000FF; color: #fff; padding: 8px 12px; text-align: left; font-weight: 500; }
    td { padding: 7px 12px; border-bottom: 1px solid #ddd; }
    tr:nth-child(even) { background: #f8f9fa; }
    .figure-placeholder { 
      padding: 15px; 
      text-align: center; 
      color: #0000FF; 
      font-weight: 500; 
      margin: 10px 0; 
      font-size: 14px;
    }
    ul { margin-left: 18px; }
    li { margin: 4px 0; font-size: 14px; }
    .bold { font-weight: 600; }
    .small { font-size: 13px; }
    .highlight { font-weight: 600; color: #0000FF; }
    code { 
      background: #f0f0f0; 
      padding: 2px 6px; 
      border-radius: 3px; 
      font-family: 'Consolas', 'Courier New', monospace; 
      font-size: 12px; 
    }
    .code-block {
      background: #f0f0f0;
      padding: 10px 14px;
      border-radius: 4px;
      font-family: 'Consolas', 'Courier New', monospace;
      font-size: 12px;
      margin: 8px 0;
      line-height: 1.5;
    }
    .gradient-banner {
      background: linear-gradient(135deg, #0000FF 0%, #6366f1 50%, #9b59b6 100%);
      color: white;
      padding: 14px 20px;
      margin: 10px 0;
      border-radius: 4px;
    }
    .gradient-banner p { color: white; }
    .slide-number {
      position: absolute;
      bottom: 8px;
      right: 15px;
      font-size: 11px;
      color: #999;
    }
  </style>
</head>
<body>

<!-- Slide 1: Title -->
<div class="slide" style="display: flex; flex-direction: column; justify-content: center; align-items: center; text-align: center;">
  <div style="margin-bottom: 30px;">
    <p style="font-size: 14px; color: #666;">Figure: AI Medical Imaging &nbsp;&nbsp;|&nbsp;&nbsp; Figure: Radiologist AI Interface</p>
  </div>
  <h1 style="color: #0000FF; font-size: 36px; font-weight: 300; letter-spacing: 1px; margin-bottom: 15px;">Medical Image Segmentation and Classification</h1>
  <p style="font-size: 20px; color: #555; font-weight: 400;">Session 07</p>
  <p style="font-size: 16px; color: #888; margin-top: 8px;">Oct 18 2025</p>
  <span class="slide-number">1</span>
</div>

<!-- Slide 2: Session Overview Part I & II -->
<div class="slide">
  <h2 class="slide-title">Session Overview</h2>
  
  <div class="blue-box" style="margin-bottom: 20px;">
    <p class="section-title" style="margin-top: 0;">Part I: Foundations & Medical Context</p>
    <ul>
      <li>Medical imaging modalities and clinical needs</li>
      <li>Problem formulation: Segmentation vs Classification</li>
      <li>Data characteristics and challenges</li>
    </ul>
  </div>
  
  <div class="green-box">
    <p class="section-title" style="margin-top: 0; color: #1abc9c;">Part II: Segmentation Architectures</p>
    <ul>
      <li>U-Net family and encoder-decoder architectures</li>
      <li>Attention mechanisms and transformers</li>
      <li>Foundation models (SAM, MedSAM)</li>
    </ul>
  </div>
  <span class="slide-number">2</span>
</div>

<!-- Slide 3: Session Overview Part III -->
<div class="slide">
  <h2 class="slide-title">Session Overview</h2>
  
  <div class="purple-box" style="margin-bottom: 20px;">
    <p class="section-title" style="margin-top: 0; color: #9b59b6;">Part III: Classification & Diagnosis</p>
    <ul>
      <li>CNN architectures to Vision Transformers</li>
      <li>Multi-modal learning and foundation models</li>
      <li>Clinical deployment and evaluation</li>
    </ul>
  </div>
  
  <div class="two-column" style="margin-top: 15px;">
    <div class="column">
      <div class="figure-placeholder" style="height: 200px;">
        Figure: Segmentation examples<br>(Liver tumor, Lung tumor, Kidney cyst, Lung mass)
      </div>
    </div>
    <div class="column">
      <div class="figure-placeholder" style="height: 200px;">
        Figure: Classification examples<br>(Retinal, Brain MRI sequences, Chest X-ray, Brain segmentation maps)
      </div>
    </div>
  </div>
  <span class="slide-number">3</span>
</div>

<!-- Slide 4: Medical Imaging Data Landscape -->
<div class="slide">
  <h2 class="slide-title">Medical Imaging: The Data Landscape</h2>
  
  <div class="two-column" style="margin-bottom: 12px;">
    <div class="column">
      <div class="blue-box">
        <p class="section-title" style="margin-top: 0;">Radiology (2D/3D)</p>
        <p><span class="bold">CT:</span> High resolution, 3D volumes</p>
        <p><span class="bold">MRI:</span> Multi-sequence, soft tissue</p>
        <p><span class="bold">X-ray:</span> 2D projection, screening</p>
        <p><span class="bold">Ultrasound:</span> Real-time, operator-dependent</p>
      </div>
    </div>
    <div class="column">
      <div class="green-box">
        <p class="section-title" style="margin-top: 0; color: #1abc9c;">Pathology (2D)</p>
        <p><span class="bold">Whole Slide Images:</span> Gigapixel resolution</p>
        <p><span class="bold">H&E Staining:</span> Standard histology</p>
        <p><span class="bold">Immunohistochemistry:</span> Molecular markers</p>
        <p><span class="bold">Multiplex imaging:</span> Multi-channel data</p>
      </div>
    </div>
  </div>
  
  <div class="cream-box">
    <p class="section-title" style="margin-top: 0; color: #f4d03f;">Key Characteristics</p>
    <div class="two-column">
      <div class="column">
        <p><span class="bold">High dimensionality</span><br><span class="small">512×512×300 voxels typical</span></p>
      </div>
      <div class="column">
        <p><span class="bold">Limited annotations</span><br><span class="small">Expert time is expensive</span></p>
      </div>
      <div class="column">
        <p><span class="bold">Class imbalance</span><br><span class="small">Pathology is rare</span></p>
      </div>
    </div>
  </div>
  
  <div class="pink-box">
    <p><span class="bold">Critical Challenge:</span> 90% of medical data is imaging, yet only ~1% is labeled for AI training</p>
  </div>
  <span class="slide-number">4</span>
</div>

<!-- Slide 5: 3D Slicer Demo -->
<div class="slide">
  <h2 class="slide-title">3D Slicer: Medical Image Visualization</h2>
  <div class="figure-placeholder" style="height: 420px;">
    Figure: 3D Slicer interface showing 3D reconstruction and segmentation overlay<br>(Femur 3D model, MRI slice views with green segmentation masks)
  </div>
  <span class="slide-number">5</span>
</div>

<!-- Slide 6: Segmentation Annotation Tool -->
<div class="slide">
  <h2 class="slide-title">Medical Image Annotation Tools</h2>
  <div class="figure-placeholder" style="height: 420px;">
    Figure: Segmentation annotation interface<br>(CT axial slice with cardiac structure segmentation overlays in blue/cyan, parameter controls panel)
  </div>
  <span class="slide-number">6</span>
</div>

<!-- Slide 7: Segmentation vs Classification -->
<div class="slide">
  <h2 class="slide-title">Segmentation vs Classification: Problem Formulation</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="blue-box">
        <p class="section-title" style="margin-top: 0;">Semantic Segmentation</p>
        <p><span class="bold">Goal:</span> Pixel/voxel-wise classification</p>
        <p><span class="bold">Output:</span> Dense prediction map (H×W×C)</p>
        <p class="bold" style="margin-top: 8px;">Applications:</p>
        <ul>
          <li>Tumor delineation for radiotherapy</li>
          <li>Organ segmentation for volume quantification</li>
          <li>Lesion identification in CT/MRI</li>
          <li>Cell nuclei segmentation in pathology</li>
        </ul>
        <div class="code-block" style="margin-top: 8px;">
          Input: (B, C, H, W, D)<br>
          Output: (B, K, H, W, D)<br>
          <span class="small">K = number of classes</span>
        </div>
      </div>
    </div>
    <div class="column">
      <div class="green-box">
        <p class="section-title" style="margin-top: 0; color: #1abc9c;">Image Classification</p>
        <p><span class="bold">Goal:</span> Image-level label prediction</p>
        <p><span class="bold">Output:</span> Class probabilities (C,)</p>
        <p class="bold" style="margin-top: 8px;">Applications:</p>
        <ul>
          <li>Disease presence/absence detection</li>
          <li>Cancer subtype classification</li>
          <li>Severity grading (mild/moderate/severe)</li>
          <li>Quality control and artifact detection</li>
        </ul>
        <div class="code-block" style="margin-top: 8px;">
          Input: (B, C, H, W, D)<br>
          Output: (B, K)<br>
          <span class="small">K = number of classes</span>
        </div>
      </div>
    </div>
  </div>
  
  <div class="cream-box" style="margin-top: 5px;">
    <p><span class="bold">Clinical Insight:</span> Segmentation provides interpretability and precise localization, while classification enables high-throughput screening. Modern systems often combine both.</p>
  </div>
  <span class="slide-number">7</span>
</div>

<!-- Slide 8: Review CNNs -->
<div class="slide">
  <h2 class="slide-title">Review: Convolutional Neural Networks</h2>
  
  <div class="two-column">
    <div class="column">
      <p class="section-title">Core Operations</p>
      
      <div class="blue-box">
        <p class="bold">Convolution</p>
        <p class="small">Local feature extraction with learned kernels</p>
        <p class="small"><code>Conv2D(in_ch, out_ch, kernel=3×3)</code></p>
      </div>
      
      <div class="green-box">
        <p class="bold">Pooling</p>
        <p class="small">Spatial downsampling (max/average)</p>
        <p class="small"><code>MaxPool2D(kernel=2×2, stride=2)</code></p>
      </div>
      
      <div class="cream-box">
        <p class="bold">Activation</p>
        <p class="small">Non-linearity (ReLU, GELU)</p>
        <p class="small"><code>ReLU(x) = max(0, x)</code></p>
      </div>
      
      <div class="purple-box">
        <p class="bold">Normalization</p>
        <p class="small">Batch/Instance/Group normalization</p>
        <p class="small"><code>BatchNorm2D(num_features)</code></p>
      </div>
    </div>
    <div class="column">
      <p class="section-title">Key Properties</p>
      
      <div style="border-left: 4px solid #0000FF; padding-left: 12px; margin: 10px 0;">
        <p class="bold">Translation Equivariance</p>
        <p class="small">If input shifts, output shifts accordingly</p>
      </div>
      
      <div style="border-left: 4px solid #1abc9c; padding-left: 12px; margin: 10px 0;">
        <p class="bold">Local Receptive Fields</p>
        <p class="small">Each neuron sees limited spatial region</p>
      </div>
      
      <div style="border-left: 4px solid #e67e22; padding-left: 12px; margin: 10px 0;">
        <p class="bold">Parameter Sharing</p>
        <p class="small">Same kernel applied across spatial locations</p>
      </div>
      
      <div style="border-left: 4px solid #9b59b6; padding-left: 12px; margin: 10px 0;">
        <p class="bold">Hierarchical Features</p>
        <p class="small">Early layers: edges → Later layers: objects</p>
      </div>
      
      <div class="pink-box" style="margin-top: 15px;">
        <p><span class="bold">Medical Imaging Adaptation:</span> 3D convolutions for volumetric data, smaller batch sizes due to memory, extensive data augmentation due to limited training data</p>
      </div>
    </div>
  </div>
  <span class="slide-number">8</span>
</div>

<!-- Slide 9: U-Net Foundation -->
<div class="slide">
  <h2 class="slide-title">U-Net: The Foundation of Medical Image Segmentation</h2>
  
  <div class="gray-box">
    <p><span class="bold">Ronneberger et al., 2015 (MICCAI)</span> - 67,000+ citations</p>
    <p class="small">Designed specifically for biomedical image segmentation with limited training data</p>
  </div>
  
  <div class="two-column" style="margin-top: 10px;">
    <div class="column">
      <p class="section-title">Architecture Components</p>
      
      <div class="orange-box">
        <p class="bold" style="color: #e67e22;">Contracting Path (Encoder)</p>
        <p class="small">Repeated: Conv 3×3 → ReLU → Conv 3×3 → ReLU → MaxPool 2×2</p>
        <p class="small">Doubles channels at each downsampling</p>
      </div>
      
      <div class="purple-box">
        <p class="bold" style="color: #9b59b6;">Bottleneck</p>
        <p class="small">Lowest spatial resolution, highest channel count</p>
        <p class="small">Captures semantic information</p>
      </div>
      
      <div class="green-box">
        <p class="bold" style="color: #1abc9c;">Expanding Path (Decoder)</p>
        <p class="small">UpConv 2×2 → Concatenate skip → Conv 3×3 → ReLU</p>
        <p class="small">Halves channels at each upsampling</p>
      </div>
    </div>
    <div class="column">
      <p class="section-title">Key Innovations</p>
      
      <div style="border-left: 4px solid #0000FF; padding-left: 12px; margin: 10px 0;">
        <p class="bold">Skip Connections</p>
        <p class="small">Concatenate encoder features with decoder</p>
        <p class="small" style="color: #e74c3c;">→ Preserves spatial information lost in downsampling</p>
      </div>
      
      <div style="border-left: 4px solid #1abc9c; padding-left: 12px; margin: 10px 0;">
        <p class="bold">Symmetric Design</p>
        <p class="small">Equal-sized encoder and decoder paths</p>
        <p class="small" style="color: #1abc9c;">→ Balances context and localization</p>
      </div>
      
      <div style="border-left: 4px solid #e67e22; padding-left: 12px; margin: 10px 0;">
        <p class="bold">Elastic Deformations</p>
        <p class="small">Aggressive data augmentation strategy</p>
        <p class="small" style="color: #e74c3c;">→ Enables training with few annotated images</p>
      </div>
    </div>
  </div>
  <span class="slide-number">9</span>
</div>

<!-- Slide 10: U-Net Architecture Diagram -->
<div class="slide">
  <h2 class="slide-title">U-Net: The Foundation of Medical Image Segmentation</h2>
  <div class="figure-placeholder" style="height: 420px;">
    Figure: U-Net architecture diagram<br>
    Encoder (left): 1→32→32, 64→64, 128→128, 256→256 channels with max pool 2×2<br>
    Decoder (right): 256→128, 128→64, 64→32→32→2 channels with up conv 2×2<br>
    Skip connections (gray arrows) connecting encoder to decoder at each level<br>
    Legend: conv 3×3 ReLU, conv 1×1, max pool 2×2, up conv 2×2, argmax
  </div>
  <span class="slide-number">10</span>
</div>

<!-- Slide 11: U-Net Visual (Encoder-Decoder U-shape) -->
<div class="slide">
  <h2 class="slide-title">U-Net: Encoder-Decoder Architecture</h2>
  <div class="figure-placeholder" style="height: 420px;">
    Figure: U-shaped encoder-decoder visualization<br>
    (Encoder on left side reducing spatial resolution, Decoder on right side recovering resolution,<br>
    Green arrows showing skip connections at each level)
  </div>
  <span class="slide-number">11</span>
</div>

<!-- Slide 12: Convolutions + Max Pooling -->
<div class="slide">
  <h2 class="slide-title">Convolutions + Max Pooling</h2>
  
  <div class="two-column">
    <div class="column">
      <p class="section-title">2D Convolution</p>
      <p class="small">Kernel size 3, stride 1, padding</p>
      <div class="figure-placeholder" style="height: 170px;">
        Figure: 2D convolution animation<br>(Input → 3×3 kernel → Output feature maps)
      </div>
      <div class="code-block">
        output = Conv2D(1, (3,3), strides=(1,1),<br>
        &nbsp;&nbsp;padding='same')(input)
      </div>
    </div>
    <div class="column">
      <p class="section-title">Max Pooling</p>
      <p class="small">Kernel size 3, stride 1, no padding</p>
      <div class="figure-placeholder" style="height: 170px;">
        Figure: Max pooling example<br>(5×4 input grid → 3×3 output with max values)
      </div>
      <div class="code-block">
        output = MaxPooling2D((3, 3), strides=(1,1),<br>
        &nbsp;&nbsp;padding='valid')(input)
      </div>
    </div>
  </div>
  <span class="slide-number">12</span>
</div>

<!-- Slide 13: Different Pooling Layers -->
<div class="slide">
  <h2 class="slide-title">Different Pooling Layers</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="blue-box" style="margin-bottom: 12px;">
        <p class="bold">Max Pool</p>
        <p class="small">Filter (2×2), Stride (2, 2)</p>
        <p class="small">Selects maximum value from each window</p>
        <p class="small">4×4 input → 2×2 output</p>
      </div>
      
      <div class="green-box">
        <p class="bold">Average Pool</p>
        <p class="small">Filter (2×2), Stride (2, 2)</p>
        <p class="small">Computes average value from each window</p>
        <p class="small">4×4 input → 2×2 output</p>
      </div>
      
      <div class="figure-placeholder" style="height: 80px;">
        Figure: Side-by-side max pool and average pool examples with colored grid cells
      </div>
    </div>
    <div class="column">
      <p class="section-title">Keras Implementation</p>
      <div class="code-block" style="font-size: 11px; line-height: 1.6;">
        import numpy as np<br>
        from keras.models import import Sequential<br>
        from keras.layers import MaxPooling2D<br><br>
        # define input image<br>
        image = np.array([[2.0, 2.0, 7.0, 3.0],<br>
        &nbsp;&nbsp;&nbsp;&nbsp;[9.0, 4.0, 6.0, 1.0],<br>
        &nbsp;&nbsp;&nbsp;&nbsp;[8.0, 5.0, 2.0, 4.0],<br>
        &nbsp;&nbsp;&nbsp;&nbsp;[3.0, 1.0, 2.0, 6.0]])<br>
        image = image.reshape(1, 4, 4, 1.0)<br><br>
        # single max pooling layer<br>
        model = Sequential(<br>
        &nbsp;&nbsp;[MaxPooling2D(pool_size=2, strides=2)])<br><br>
        output = model.predict(image)<br>
        output = np.squeeze(output)<br>
        print(output)
      </div>
      <p class="small" style="margin-top: 5px; color: #666;">
        Source: <a href="https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/" style="color: #0000FF;">geeksforgeeks.org</a>
      </p>
    </div>
  </div>
  <span class="slide-number">13</span>
</div>

<!-- Slide 14: Transpose Convolution -->
<div class="slide">
  <h2 class="slide-title">Transpose Convolution - Upsampling</h2>
  
  <div class="blue-box" style="margin-bottom: 15px;">
    <p><span class="bold">Purpose:</span> Increase spatial resolution (opposite of regular convolution with stride > 1). Essential for the decoder path in U-Net.</p>
  </div>
  
  <div class="two-column">
    <div class="column">
      <div class="figure-placeholder" style="height: 260px;">
        Figure: Transposed 2D convolution with padding,<br>stride of 2 and kernel of 3<br>
        (Small input → Large output, showing how values are spread and accumulated)
      </div>
      <div class="code-block">
        output = Conv2DTranspose(1, (3,3),<br>
        &nbsp;&nbsp;strides=(2,2), padding='same')(input)
      </div>
    </div>
    <div class="column">
      <div class="figure-placeholder" style="height: 260px;">
        Figure: Second view of transpose convolution<br>
        (Demonstrating overlapping regions and value accumulation)
      </div>
    </div>
  </div>
  <span class="slide-number">14</span>
</div>

<!-- Slide 15: Transpose Convolution - Andrew Ng -->
<div class="slide">
  <h2 class="slide-title">Transpose Convolutions Explained</h2>
  
  <div class="figure-placeholder" style="height: 380px;">
    Figure: Andrew Ng's transpose convolution explanation<br><br>
    2×2 input → 4×4 output using 3×3 filter<br>
    filter f×f = 3×3 &nbsp;&nbsp; padding p = 1 &nbsp;&nbsp; stride s = 2<br><br>
    Shows step-by-step multiplication of each input element with the filter kernel,<br>
    placing results in the output grid with stride spacing,<br>
    and accumulating overlapping values
  </div>
  
  <div class="cream-box">
    <p><span class="bold">Output size formula:</span> o = s(i - 1) + f - 2p &nbsp;&nbsp; where i=input size, f=filter size, p=padding, s=stride</p>
  </div>
  <span class="slide-number">15</span>
</div>

<!-- Slide 16: UNet Encoder Overview -->
<div class="slide">
  <h2 class="slide-title">UNet: Encoder</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="figure-placeholder" style="height: 300px;">
        Figure: Full U-Net architecture with encoder path highlighted<br>
        (Orange/peach shading on left side showing contracting path)
      </div>
    </div>
    <div class="column">
      <p class="section-title">Encoder Block Detail</p>
      <div class="figure-placeholder" style="height: 150px;">
        Figure: Single encoder block<br>
        (Input → Conv 3×3 ReLU → Conv 3×3 ReLU → [enc_layer output + MaxPool 2×2 output])
      </div>
      
      <div class="blue-box">
        <p class="bold">Operations per block:</p>
        <p class="small">1. Two successive 3×3 convolutions with ReLU</p>
        <p class="small">2. Skip connection output (enc_layer) saved</p>
        <p class="small">3. Max pooling 2×2 for downsampling</p>
      </div>
    </div>
  </div>
  <span class="slide-number">16</span>
</div>

<!-- Slide 17: UNet Encoder Code -->
<div class="slide">
  <h2 class="slide-title">UNet: Encoder</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="figure-placeholder" style="height: 300px;">
        Figure: Encoder block diagram<br>
        inputs (N×N) → conv 3×3 ReLU → conv 3×3 ReLU → enc_layer<br>
        ↓ max pool 2×2<br>
        outputs (N²/4)
      </div>
      <div style="margin-top: 10px;">
        <span style="color: #0000FF;">▶</span> <span class="bold">conv 3×3 ReLU</span><br>
        <span style="color: #e74c3c;">▼</span> <span class="bold" style="color: #e74c3c;">max pool 2×2</span>
      </div>
    </div>
    <div class="column">
      <p class="section-title">Keras Implementation</p>
      <div class="code-block" style="font-size: 11px; line-height: 1.7;">
        c = Conv2D(filters, (3,3), activation='relu',<br>
        &nbsp;&nbsp;kernel_initializer=kernel_initializer,<br>
        &nbsp;&nbsp;padding='same')(inputs)<br><br>
        c = Conv2D(filters, (3,3), activation='relu',<br>
        &nbsp;&nbsp;kernel_initializer=kernel_initializer,<br>
        &nbsp;&nbsp;padding='same')(c)<br><br>
        enc_layer = c<br><br>
        <span style="color: #e74c3c;">outputs = MaxPooling2D((2, 2))(c)</span>
      </div>
      
      <div class="cream-box" style="margin-top: 10px;">
        <p class="small"><span class="bold">Key:</span> The <code>enc_layer</code> is saved before pooling and later concatenated with the decoder via skip connections</p>
      </div>
    </div>
  </div>
  <span class="slide-number">17</span>
</div>

<!-- Slide 18: UNet Decoder Overview -->
<div class="slide">
  <h2 class="slide-title">UNet: Decoder</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="figure-placeholder" style="height: 300px;">
        Figure: Full U-Net architecture with decoder path highlighted<br>
        (Green shading on right side showing expanding path)
      </div>
    </div>
    <div class="column">
      <p class="section-title">Decoder Block Detail</p>
      <div class="figure-placeholder" style="height: 150px;">
        Figure: Single decoder block<br>
        (Input → Up Conv 2×2 → Concatenate with enc_layer → Conv 3×3 ReLU → Conv 3×3 ReLU → Output)
      </div>
      
      <div class="green-box">
        <p class="bold">Operations per block:</p>
        <p class="small">1. Transposed convolution (up conv 2×2) to upsample</p>
        <p class="small">2. Concatenate with corresponding encoder features</p>
        <p class="small">3. Two successive 3×3 convolutions with ReLU</p>
      </div>
    </div>
  </div>
  <span class="slide-number">18</span>
</div>

<!-- Slide 19: UNet Decoder Code -->
<div class="slide">
  <h2 class="slide-title">UNet: Decoder</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="figure-placeholder" style="height: 280px;">
        Figure: Decoder block diagram<br>
        inputs → up conv 2×2 ↑<br>
        + enc_layer (skip connection) →<br>
        conv 3×3 ReLU → conv 3×3 ReLU → outputs (N²/4)
      </div>
      <div style="margin-top: 10px;">
        <span style="color: #0000FF;">▶</span> <span class="bold">conv 3×3 ReLU</span><br>
        <span style="color: #1abc9c;">▲</span> <span class="bold" style="color: #1abc9c;">up conv 2×2</span><br>
        <span style="color: #999;">→</span> <span class="bold">copy (skip connection)</span>
      </div>
    </div>
    <div class="column">
      <p class="section-title">Keras Implementation</p>
      <div class="code-block" style="font-size: 11px; line-height: 1.7;">
        <span style="color: #1abc9c;">c = Conv2DTranspose(filters, (2, 2),<br>
        &nbsp;&nbsp;strides=(2, 2), padding='same')(input)</span><br><br>
        <span style="color: #888;">c = Concatenate()([c, enc_layer])</span><br><br>
        c = Conv2D(filters, (3,3), activation='relu',<br>
        &nbsp;&nbsp;kernel_initializer=kernel_initializer,<br>
        &nbsp;&nbsp;padding='same')(inputs)<br><br>
        outputs = Conv2D(filters, (3,3), activation='relu',<br>
        &nbsp;&nbsp;kernel_initializer=kernel_initializer,<br>
        &nbsp;&nbsp;padding='same')(c)
      </div>
    </div>
  </div>
  <span class="slide-number">19</span>
</div>

<!-- Slide 20: UNet Decoder Output -->
<div class="slide">
  <h2 class="slide-title">UNet: Decoder - Final Output Layer</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="figure-placeholder" style="height: 280px;">
        Figure: Full U-Net with output layer highlighted<br>
        (Final 1×1 convolution mapping to num_classes channels,<br>
        followed by argmax for segmentation map)
      </div>
    </div>
    <div class="column">
      <p class="section-title">Output Layer</p>
      
      <div class="figure-placeholder" style="height: 120px;">
        Figure: Output block detail<br>
        inputs → conv 1×1 → argmax → segmentation map (num_classes)
      </div>
      
      <div class="code-block" style="margin-top: 15px;">
        outputs = Conv2D(num_classes, (1, 1),<br>
        &nbsp;&nbsp;activation='sigmoid')(input)
      </div>
      
      <div class="cream-box" style="margin-top: 10px;">
        <p class="small"><span class="bold">Note:</span> The argmax is done outside the network (loss or metric, display, ...)</p>
      </div>
      
      <div style="margin-top: 10px;">
        <span style="color: #6366f1;">◇</span> <span class="bold">conv 1×1</span><br>
        <span style="color: #f59e0b;">▶</span> <span class="bold" style="color: #f59e0b;">argmax</span>
      </div>
    </div>
  </div>
  <span class="slide-number">20</span>
</div>

<!-- Slide 21: U-Net Evolution -->
<div class="slide">
  <h2 class="slide-title">U-Net Evolution: Modern Variants</h2>
  
  <div class="two-column" style="margin-bottom: 10px;">
    <div class="column">
      <div class="blue-box">
        <p class="bold" style="color: #0000FF;">3D U-Net (2016)</p>
        <p class="small">Extends to volumetric medical data</p>
        <ul class="small">
          <li>3D convolutions and 3D pooling</li>
          <li>Better for CT/MRI volumes</li>
          <li>Memory intensive: patch-based training</li>
        </ul>
      </div>
      
      <div class="purple-box">
        <p class="bold" style="color: #9b59b6;">U-Net++ (2018)</p>
        <p class="small">Dense skip pathways</p>
        <ul class="small">
          <li>Nested and dense skip connections</li>
          <li>Reduces semantic gap between encoder-decoder</li>
          <li>Deep supervision at multiple scales</li>
        </ul>
      </div>
    </div>
    <div class="column">
      <div class="green-box">
        <p class="bold" style="color: #1abc9c;">Attention U-Net (2018)</p>
        <p class="small">Attention gates in skip connections</p>
        <ul class="small">
          <li>Learns to focus on relevant features</li>
          <li>Suppresses irrelevant regions</li>
          <li>Improved boundary delineation</li>
        </ul>
      </div>
      
      <div class="red-box">
        <p class="bold" style="color: #e74c3c;">nnU-Net (2021)</p>
        <p class="small">Self-configuring framework</p>
        <ul class="small">
          <li>Automated architecture adaptation</li>
          <li>Dataset-specific preprocessing</li>
          <li>State-of-art on Medical Segmentation Decathlon</li>
        </ul>
      </div>
    </div>
  </div>
  
  <div class="cream-box">
    <p class="bold">nnU-Net: Key Principles</p>
    <div class="two-column">
      <div class="column"><p class="small"><span class="bold">Automatic Preprocessing</span><br>Resampling, normalization based on data properties</p></div>
      <div class="column"><p class="small"><span class="bold">Dynamic Architecture</span><br>2D, 3D, or cascade based on dataset</p></div>
      <div class="column"><p class="small"><span class="bold">Robust Training</span><br>Optimized augmentation and loss functions</p></div>
    </div>
  </div>
  <span class="slide-number">21</span>
</div>

<!-- Slide 22: Attention Mechanisms & Vision Transformers -->
<div class="slide">
  <h2 class="slide-title">Attention Mechanisms & Vision Transformers</h2>
  
  <div class="gray-box" style="margin-bottom: 10px;">
    <p class="bold">From Local to Global Context</p>
    <p class="small">CNNs have limited receptive fields. Transformers enable global context modeling through self-attention.</p>
  </div>
  
  <div class="two-column">
    <div class="column">
      <p class="section-title">Self-Attention Mechanism</p>
      <div class="code-block">
        Attention(Q, K, V) = softmax(QK^T / √d_k)V
      </div>
      <p style="margin: 8px 0;"><span class="bold">Query (Q):</span> What am I looking for?</p>
      <p><span class="bold">Key (K):</span> What do I contain?</p>
      <p><span class="bold">Value (V):</span> What information do I have?</p>
      
      <div class="blue-box" style="margin-top: 10px;">
        <p><span class="bold">Advantage:</span> Each position attends to all other positions, capturing long-range dependencies</p>
      </div>
    </div>
    <div class="column">
      <p class="section-title">Vision Transformer (ViT)</p>
      <div class="green-box"><p class="small"><span class="bold">1. Patch Embedding</span><br>Split image into patches (16×16), linear projection</p></div>
      <div class="blue-box"><p class="small"><span class="bold">2. Position Encoding</span><br>Add learnable position embeddings</p></div>
      <div class="cream-box"><p class="small"><span class="bold">3. Transformer Encoder</span><br>Stack of self-attention + MLP blocks</p></div>
      <div class="purple-box"><p class="small"><span class="bold">4. Classification Head</span><br>MLP on [CLS] token or global pooling</p></div>
    </div>
  </div>
  
  <div class="pink-box" style="margin-top: 5px;">
    <p class="bold" style="color: #ec4899;">Challenges in Medical Imaging</p>
    <div class="two-column">
      <div class="column"><p class="small"><span class="bold">Data Hungry</span> — ViT requires large datasets or pre-training</p></div>
      <div class="column"><p class="small"><span class="bold">Computational Cost</span> — O(n²) complexity for self-attention</p></div>
      <div class="column"><p class="small"><span class="bold">Loss of Inductive Bias</span> — No built-in translation equivariance</p></div>
    </div>
  </div>
  <span class="slide-number">22</span>
</div>

<!-- Slide 23: Hybrid Architectures -->
<div class="slide">
  <h2 class="slide-title">Hybrid Architectures: TransUNet & Swin-UNet</h2>
  
  <div class="two-column">
    <div class="column">
      <div class="blue-box">
        <p class="bold" style="color: #0000FF; font-size: 16px;">TransUNet (2021)</p>
        <p class="small">Combines CNN and Transformer for segmentation</p>
        
        <div style="background: #f0f0ff; padding: 6px 10px; margin: 6px 0; border-radius: 3px;">
          <p class="small"><span class="bold">Encoder</span> — CNN backbone (ResNet) for low-level features</p>
        </div>
        <div style="background: #e8e0f0; padding: 6px 10px; margin: 6px 0; border-radius: 3px;">
          <p class="small"><span class="bold">Bottleneck</span> — Transformer encoder for global context</p>
        </div>
        <div style="background: #e0f0e8; padding: 6px 10px; margin: 6px 0; border-radius: 3px;">
          <p class="small"><span class="bold">Decoder</span> — Cascade upsampling with skip connections</p>
        </div>
        
        <div class="cream-box">
          <p class="small"><span class="bold">Benefit:</span> CNN captures local texture, Transformer captures global semantic relationships</p>
        </div>
      </div>
    </div>
    <div class="column">
      <div class="purple-box">
        <p class="bold" style="color: #9b59b6; font-size: 16px;">Swin-UNet (2022)</p>
        <p class="small">Pure transformer with hierarchical design</p>
        
        <div style="background: #f8f0ff; padding: 6px 10px; margin: 6px 0; border-radius: 3px;">
          <p class="small"><span class="bold">Shifted Windows</span> — Local attention within windows, then shifted</p>
        </div>
        <div style="background: #f0e8ff; padding: 6px 10px; margin: 6px 0; border-radius: 3px;">
          <p class="small"><span class="bold">Patch Merging</span> — Hierarchical feature maps (like CNN pooling)</p>
        </div>
        <div style="background: #e8e0ff; padding: 6px 10px; margin: 6px 0; border-radius: 3px;">
          <p class="small"><span class="bold">Linear Complexity</span> — O(n) instead of O(n²) for self-attention</p>
        </div>
        
        <div class="cream-box">
          <p class="small"><span class="bold">Advantage:</span> Efficient for high-resolution medical images, maintains hierarchical representation</p>
        </div>
      </div>
    </div>
  </div>
  <span class="slide-number">23</span>
</div>

<!-- Slide 24: Performance Comparison -->
<div class="slide">
  <h2 class="slide-title">Hybrid Architectures: TransUNet & Swin-UNet</h2>
  
  <div class="gray-box">
    <p class="bold">Performance Comparison</p>
  </div>
  
  <table style="margin-top: 15px; font-size: 15px;">
    <thead>
      <tr>
        <th>Architecture</th>
        <th>Synapse Multi-Organ</th>
        <th>Parameters</th>
        <th>Key Strength</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><span class="bold">U-Net</span></td>
        <td>76.85% DSC</td>
        <td>~31M</td>
        <td>Simple, reliable</td>
      </tr>
      <tr>
        <td><span class="bold">TransUNet</span></td>
        <td>77.48% DSC</td>
        <td>~105M</td>
        <td>Global context</td>
      </tr>
      <tr>
        <td><span class="bold">Swin-UNet</span></td>
        <td>79.13% DSC</td>
        <td>~27M</td>
        <td>Efficiency</td>
      </tr>
    </tbody>
  </table>
  
  <div class="cream-box" style="margin-top: 30px;">
    <p><span class="bold">Key Takeaway:</span> Swin-UNet achieves the best performance with fewer parameters than TransUNet, demonstrating the effectiveness of hierarchical windowed attention for medical image segmentation.</p>
  </div>
  <span class="slide-number">24</span>
</div>

<!-- Slide 25: Foundation Models - SAM -->
<div class="slide">
  <h2 class="slide-title">Foundation Models: Segment Anything (SAM)</h2>
  
  <div class="gradient-banner">
    <p class="bold" style="font-size: 18px;">Meta AI - April 2023</p>
    <p>First foundation model for image segmentation trained on 1 billion masks</p>
  </div>
  
  <div class="two-column" style="margin-top: 10px;">
    <div class="column">
      <p class="section-title">Architecture Overview</p>
      
      <div class="blue-box">
        <p class="bold" style="color: #0000FF;">Image Encoder</p>
        <p class="small">ViT-H (Huge): 632M parameters</p>
        <p class="small">Pre-computable embeddings for efficiency</p>
      </div>
      
      <div class="green-box">
        <p class="bold" style="color: #1abc9c;">Prompt Encoder</p>
        <p class="small">Points, boxes, masks, or text</p>
        <p class="small">Flexible user interaction</p>
      </div>
      
      <div class="purple-box">
        <p class="bold" style="color: #9b59b6;">Mask Decoder</p>
        <p class="small">Lightweight transformer</p>
        <p class="small">Predicts multiple mask proposals</p>
      </div>
      
      <div class="gray-box">
        <p class="small"><span class="bold">Key Innovation:</span> Promptable segmentation enables zero-shot transfer to new domains</p>
      </div>
    </div>
    <div class="column">
      <p class="section-title">Prompting Strategies</p>
      
      <div style="border-left: 4px solid #0000FF; padding-left: 12px; margin: 8px 0;">
        <p class="bold">Point Prompts</p>
        <p class="small">Click on object of interest</p>
        <p class="small" style="color: #1abc9c;">Best for: Interactive annotation</p>
      </div>
      
      <div style="border-left: 4px solid #e67e22; padding-left: 12px; margin: 8px 0;">
        <p class="bold">Box Prompts</p>
        <p class="small">Bounding box around region</p>
        <p class="small" style="color: #1abc9c;">Best for: Object detection pipelines</p>
      </div>
      
      <div style="border-left: 4px solid #9b59b6; padding-left: 12px; margin: 8px 0;">
        <p class="bold">Mask Prompts</p>
        <p class="small">Coarse mask refinement</p>
        <p class="small" style="color: #1abc9c;">Best for: Iterative segmentation</p>
      </div>
      
      <div style="border-left: 4px solid #e74c3c; padding-left: 12px; margin: 8px 0;">
        <p class="bold">Automatic Mode</p>
        <p class="small">Grid of points for everything</p>
        <p class="small" style="color: #1abc9c;">Best for: Dense segmentation</p>
      </div>
    </div>
  </div>
  <span class="slide-number">25</span>
</div>

<!-- Slide 26: MedSAM -->
<div class="slide">
  <h2 class="slide-title">MedSAM: Medical Adaptation of SAM</h2>
  
  <div class="gradient-banner" style="background: linear-gradient(135deg, #1abc9c 0%, #0000FF 100%);">
    <p class="bold" style="font-size: 18px;">MedSAM (Nature Communications, 2024)</p>
    <p>Fine-tuned SAM on 1.6M medical image-mask pairs across 11 modalities</p>
  </div>
  
  <div class="two-column" style="margin-top: 10px;">
    <div class="column">
      <p class="section-title">Training Strategy</p>
      
      <div class="blue-box">
        <p class="bold">Dataset Composition</p>
        <ul class="small">
          <li>CT: 760K images (tumors, organs)</li>
          <li>MRI: 480K images (brain, cardiac)</li>
          <li>Ultrasound: 150K images</li>
          <li>Microscopy: 210K images (cells, tissues)</li>
        </ul>
      </div>
      
      <div class="green-box">
        <p class="bold">Fine-tuning Approach</p>
        <p class="small">Freeze image encoder, train mask decoder and prompt encoder on medical data</p>
        <p class="small" style="color: #1abc9c;">Preserves natural image features, adapts to medical domain</p>
      </div>
    </div>
    <div class="column">
      <p class="section-title">Performance Improvements</p>
      
      <table>
        <thead>
          <tr>
            <th>Task</th>
            <th>SAM</th>
            <th>MedSAM</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Liver CT</td>
            <td>67.1%</td>
            <td><span class="bold" style="color: #1abc9c;">91.8%</span></td>
          </tr>
          <tr>
            <td>Brain MRI</td>
            <td>48.2%</td>
            <td><span class="bold" style="color: #1abc9c;">84.3%</span></td>
          </tr>
          <tr>
            <td>Cell Nuclei</td>
            <td>72.5%</td>
            <td><span class="bold" style="color: #1abc9c;">89.7%</span></td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
  <span class="slide-number">26</span>
</div>

<!-- Slide 27: MedSAM Clinical Workflow -->
<div class="slide">
  <h2 class="slide-title">MedSAM: Medical Adaptation of SAM</h2>
  
  <div class="green-box" style="margin-bottom: 15px;">
    <p class="bold">Fine-tuning Approach</p>
    <p class="small">Freeze image encoder, train mask decoder and prompt encoder on medical data</p>
    <p class="small" style="color: #1abc9c;">Preserves natural image features, adapts to medical domain</p>
  </div>
  
  <div class="gray-box" style="margin-bottom: 15px;">
    <p class="bold">Clinical Workflow Integration</p>
    <div class="two-column" style="margin-top: 8px;">
      <div class="column">
        <p class="bold" style="color: #0000FF;">Interactive Refinement</p>
        <p class="small">Radiologist provides box prompt → MedSAM segments → Add/remove points to refine</p>
      </div>
      <div class="column">
        <p class="bold" style="color: #1abc9c;">Quality Control</p>
        <p class="small">Rapid review of automated segmentations with minimal corrections needed</p>
      </div>
      <div class="column">
        <p class="bold" style="color: #9b59b6;">Training Data Creation</p>
        <p class="small">10x faster annotation for building task-specific models</p>
      </div>
    </div>
  </div>
  
  <div style="background: linear-gradient(135deg, #1a1a5e 0%, #0000AA 100%); color: white; padding: 14px 20px; border-radius: 4px;">
    <p><span class="bold">Open Source:</span> MedSAM models and code available at github.com/bowang-lab/MedSAM</p>
  </div>
  <span class="slide-number">27</span>
</div>

<!-- Slide 28: Loss Functions for Segmentation -->
<div class="slide">
  <h2 class="slide-title">Loss Functions for Segmentation</h2>
  
  <div class="two-column" style="margin-bottom: 10px;">
    <div class="column">
      <div class="blue-box">
        <p class="bold" style="color: #0000FF;">Cross-Entropy Loss</p>
        <div class="code-block" style="margin: 6px 0;">L = -Σ y_i log(p_i)</div>
        <p class="small">Pixel-wise classification loss</p>
        <div style="background: #e8f8f5; padding: 5px 8px; margin: 4px 0; border-radius: 3px;">
          <p class="small"><span class="bold" style="color: #1abc9c;">Pros:</span> Well-studied, stable training | Works with multi-class problems</p>
        </div>
        <div style="background: #fdeaea; padding: 5px 8px; margin: 4px 0; border-radius: 3px;">
          <p class="small"><span class="bold" style="color: #e74c3c;">Cons:</span> Sensitive to class imbalance | Treats pixels independently</p>
        </div>
      </div>
    </div>
    <div class="column">
      <div class="green-box">
        <p class="bold" style="color: #1abc9c;">Dice Loss</p>
        <div class="code-block" style="margin: 6px 0;">L = 1 - (2|X∩Y|)/(|X|+|Y|)</div>
        <p class="small">Based on Dice similarity coefficient</p>
        <div style="background: #e8f8f5; padding: 5px 8px; margin: 4px 0; border-radius: 3px;">
          <p class="small"><span class="bold" style="color: #1abc9c;">Pros:</span> Handles class imbalance naturally | Directly optimizes evaluation metric</p>
        </div>
        <div style="background: #fdeaea; padding: 5px 8px; margin: 4px 0; border-radius: 3px;">
          <p class="small"><span class="bold" style="color: #e74c3c;">Cons:</span> Can be unstable for small objects | Non-convex optimization surface</p>
        </div>
      </div>
    </div>
  </div>
  
  <div class="two-column">
    <div class="column">
      <div class="purple-box">
        <p class="bold" style="color: #9b59b6;">Focal Loss</p>
        <div class="code-block" style="margin: 6px 0;">L = -α(1-p)^γ log(p)</div>
        <p class="small">Down-weights easy examples</p>
        <div class="cream-box" style="margin: 4px 0; padding: 5px 8px;">
          <p class="small"><span class="bold">Use Case:</span> Extreme class imbalance, small object detection, hard example mining</p>
        </div>
      </div>
    </div>
    <div class="column">
      <div class="orange-box">
        <p class="bold" style="color: #e67e22;">Boundary Loss</p>
        <div class="code-block" style="margin: 6px 0;">L = ∫ φ(s)ds</div>
        <p class="small">Penalizes boundary errors</p>
        <div class="cream-box" style="margin: 4px 0; padding: 5px 8px;">
          <p class="small"><span class="bold">Use Case:</span> Precise boundary delineation, organ segmentation, tumor margin definition</p>
        </div>
      </div>
    </div>
  </div>
  <span class="slide-number">28</span>
</div>

</body>
</html>
